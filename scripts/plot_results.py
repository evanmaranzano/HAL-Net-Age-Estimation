import pandas as pd
import matplotlib.pyplot as plt
import os
import numpy as np
import seaborn as sns
import glob

# ================= é…ç½®åŒºåŸŸ =================
SAVE_DIR = 'plots'
plt.style.use('seaborn-v0_8-paper')

# ç»Ÿä¸€çš„è®ºæ–‡çº§å­—ä½“é…ç½®
plt.rcParams.update({
    'font.family': 'serif',
    'font.serif': ['Times New Roman'],
    'axes.unicode_minus': False,
    'figure.dpi': 300,
    'axes.labelsize': 14,
    'axes.titlesize': 16,
    'axes.titleweight': 'bold',
    'legend.fontsize': 12,
    'xtick.labelsize': 11,
    'ytick.labelsize': 11,
    'lines.linewidth': 2.5 
})

def load_real_data(seed=None):
    # Determine filenames based on seed
    if seed:
        file_epoch = f'training_log_seed{seed}.csv'
        file_batch = f'batch_log_seed{seed}.csv'
    else:
        # Default mode: Try standard file first
        if os.path.exists('training_log.csv'):
            file_epoch = 'training_log.csv'
            file_batch = 'batch_log.csv'
            seed = 'Legacy'
        else:
            # Auto-detect latest seed log
            print("âš ï¸ No seed specified and 'training_log.csv' not found.")
            logs = glob.glob('training_log_seed*.csv')
            if not logs:
                print("âŒ No training logs found in current directory.")
                return None, None
                
            # Sort by modification time (newest first)
            latest_log = max(logs, key=os.path.getmtime)
            # Extract seed from filename "training_log_seedXXXX.csv"
            try:
                detected_seed = latest_log.replace('training_log_seed', '').replace('.csv', '')
                print(f"ğŸ•µï¸ Auto-detected latest run: Seed {detected_seed}")
                file_epoch = latest_log
                file_batch = f'batch_log_seed{detected_seed}.csv'
                seed = detected_seed
            except:
                print("âŒ Failed to parse seed from filename.")
                return None, None

    print(f"ğŸ” Loading logs: {file_epoch}")

    if not (os.path.exists(file_epoch) and os.path.exists(file_batch)):
        print(f"âŒ Error: Batch log not found for seed {seed}")
        print(f"   Expected: {file_batch}")
        return None, None
        
    print(f"ğŸ“‚ Reading data...")
    df_epoch = pd.read_csv(file_epoch)
    df_batch = pd.read_csv(file_batch)
    return df_epoch, df_batch, seed

def add_best_model_line(ax_plt, epoch, label_y_pos=None, color='#333333'):
    """è¾…åŠ©å‡½æ•°ï¼šæ·»åŠ æœ€ä½³æ¨¡å‹å‚ç›´çº¿"""
    ax_plt.axvline(x=epoch, color=color, linestyle='--', linewidth=1.5, alpha=0.7)
    
    ymin, ymax = ax_plt.get_ylim()
    text_pos = ymax - (ymax - ymin) * 0.05 if label_y_pos is None else label_y_pos
    
    ax_plt.text(epoch, text_pos, ' Best Checkpoint', rotation=90, 
                verticalalignment='top', fontsize=10, color=color, alpha=0.8)

def plot_thesis_suite(seed=None):
    df_epoch, df_batch, detected_seed = load_real_data(seed)
    if df_epoch is None: return

    # Use detected seed if original seed was None
    final_seed = seed if seed else detected_seed

    # Dynamic Save Dir
    if final_seed and final_seed != 'Legacy':
        current_save_dir = os.path.join(SAVE_DIR, f"seed_{final_seed}")
    else:
        current_save_dir = SAVE_DIR
        
    if not os.path.exists(current_save_dir):
        os.makedirs(current_save_dir)

    # è®¡ç®—å…¨å±€æœ€ä½³è½®æ¬¡ (åŸºäº Val_MAE)
    best_idx = df_epoch['Val_MAE'].idxmin()
    best_epoch = df_epoch.loc[best_idx, 'Epoch']
    best_mae_val = df_epoch.loc[best_idx, 'Val_MAE']
    
    print(f"ğŸš€ å¼€å§‹ç”Ÿæˆ 8 å¼ ç‹¬ç«‹å›¾è¡¨ -> {SAVE_DIR}/")
    print(f"ğŸ’¡ æœ€ä½³æ¨¡å‹å‡ºç°åœ¨ç¬¬ {best_epoch} è½® (MAE={best_mae_val:.4f})")

    # ==========================================
    # å›¾ 1: Loss æ”¶æ•›æ›²çº¿ (åŸºç¡€ç‰ˆ)
    # ==========================================
    plt.figure(figsize=(8, 6))
    ax1 = plt.gca()
    plt.plot(df_epoch['Epoch'], df_epoch['Train_Loss'], label='Train Loss', color='#2878B5')
    plt.plot(df_epoch['Epoch'], df_epoch['Val_Loss'], label='Val Loss', color='#D76364', linestyle='--')
    add_best_model_line(ax1, best_epoch)
    plt.title('Loss Convergence')
    plt.xlabel('Epoch')
    plt.ylabel('Loss')
    plt.legend(frameon=True, fancybox=True)
    plt.grid(True, linestyle='--', alpha=0.5)
    plt.tight_layout()
    plt.savefig(f'{current_save_dir}/1_loss_curve.png')
    plt.close()

    # ==========================================
    # å›¾ 2: MAE æ€§èƒ½æ›²çº¿
    # ==========================================
    plt.figure(figsize=(8, 6))
    plt.plot(df_epoch['Epoch'], df_epoch['Train_MAE'], label='Train MAE', color='#9AC9DB')
    plt.plot(df_epoch['Epoch'], df_epoch['Val_MAE'], label='Val MAE', color='#C82423', linestyle='--')
    plt.axvline(x=best_epoch, color='gray', linestyle='--', linewidth=1.5, alpha=0.6)
    plt.scatter(best_epoch, best_mae_val, color='black', s=60, zorder=5)
    plt.annotate(f'Best MAE: {best_mae_val:.2f}\n(Epoch {best_epoch})', 
                 xy=(best_epoch, best_mae_val), 
                 xytext=(best_epoch + 5, best_mae_val + 0.5),
                 arrowprops=dict(facecolor='black', arrowstyle='->'),
                 fontsize=12, fontweight='bold',
                 bbox=dict(boxstyle='round,pad=0.2', fc='white', alpha=0.8))
    plt.title('Model Performance (MAE)')
    plt.xlabel('Epoch')
    plt.ylabel('Mean Absolute Error')
    plt.legend(frameon=True, fancybox=True)
    plt.grid(True, linestyle='--', alpha=0.5)
    plt.tight_layout()
    plt.savefig(f'{current_save_dir}/2_mae_curve.png')
    plt.close()

    # ==========================================
    # å›¾ 3: å­¦ä¹ ç‡è°ƒåº¦
    # ==========================================
    plt.figure(figsize=(8, 4))
    plt.plot(df_epoch['Epoch'], df_epoch['LR'], color='#6D6D6D', alpha=0.8)
    plt.fill_between(df_epoch['Epoch'], df_epoch['LR'], color='#6D6D6D', alpha=0.1)
    plt.axvline(x=best_epoch, color='gray', linestyle=':', linewidth=1, alpha=0.5)
    plt.title('Learning Rate Schedule')
    plt.xlabel('Epoch')
    plt.ylabel('Learning Rate (log scale)')
    plt.yscale('log')
    plt.grid(True, which="both", ls="--", alpha=0.3)
    plt.tight_layout()
    plt.savefig(f'{current_save_dir}/3_lr_schedule.png')
    plt.close()

    # ==========================================
    # å›¾ 4: æ³›åŒ–å·®è·åˆ†æ
    # ==========================================
    gap = df_epoch['Val_Loss'] - df_epoch['Train_Loss']
    plt.figure(figsize=(8, 5))
    ax4 = plt.gca()
    plt.plot(df_epoch['Epoch'], gap, color='#845EC2', label='Generalization Gap')
    plt.fill_between(df_epoch['Epoch'], gap, 0, color='#845EC2', alpha=0.15)
    z = np.polyfit(df_epoch['Epoch'], gap, 1)
    p = np.poly1d(z)
    plt.plot(df_epoch['Epoch'], p(df_epoch['Epoch']), "k--", alpha=0.5, linewidth=1, label='Gap Trend')
    add_best_model_line(ax4, best_epoch)
    plt.title('Generalization Gap Dynamics')
    plt.xlabel('Epoch')
    plt.ylabel('Loss Difference ($Val - Train$)')
    plt.legend(loc='upper left')
    plt.grid(True, linestyle='--', alpha=0.5)
    plt.tight_layout()
    plt.savefig(f'{current_save_dir}/4_generalization_gap.png')
    plt.close()

    # ==========================================
    # å›¾ 5: Batch ç¨³å®šæ€§ (è¶‹åŠ¿å›¾)
    # ==========================================
    plt.figure(figsize=(12, 4))
    global_steps = range(len(df_batch))
    plt.plot(global_steps, df_batch['Total_Loss'], color='#555555', alpha=0.3, linewidth=0.5, label='Raw Batch Loss')
    window = 100
    if len(df_batch) > window:
        trend = df_batch['Total_Loss'].rolling(window).mean()
        plt.plot(global_steps, trend, color='#C82423', linewidth=1.5, label=f'Trend (MA={window})')
    plt.title('Training Stability (Batch Level)')
    plt.xlabel('Global Step')
    plt.ylabel('Loss')
    limit = df_batch['Total_Loss'].iloc[int(len(df_batch)*0.01):].quantile(0.999) * 1.1
    plt.ylim(0, limit)
    plt.legend(loc='upper right', frameon=True)
    plt.margins(x=0)
    plt.tight_layout()
    plt.savefig(f'{current_save_dir}/5_batch_stability.png')
    plt.close()

    # ==========================================
    # [NEW] å›¾ 6: è®­ç»ƒæ—¶é—´æ•ˆç‡åˆ†æ (Time Efficiency)
    # ==========================================
    # è®¡ç®—æ¯ä¸ª Epoch çš„è€—æ—¶ (å¤„ç†æ–­ç‚¹ç»­è®­çš„æƒ…å†µ)
    time_deltas = []
    prev_time = 0
    print("\nâ±ï¸ æ­£åœ¨åˆ†æè®­ç»ƒè€—æ—¶ (æ£€æµ‹æ–­ç‚¹)...")
    for idx, t in enumerate(df_epoch['Time']):
        epoch_num = df_epoch.loc[idx, 'Epoch']
        if t < prev_time: # å‘ç”Ÿäº†é‡å¯
            delta = t
            print(f"  -> Epoch {epoch_num}: æ£€æµ‹åˆ°æ—¶é—´é‡ç½® (Time={t:.1f}s) -> åˆ¤å®šä¸ºé‡å¯åé¦–è½®")
        else:
            delta = t - prev_time
        
        time_deltas.append(delta)
        prev_time = t
    
    avg_time = np.mean(time_deltas)
    print(f"  -> å¹³å‡æ¯è½®è€—æ—¶: {avg_time:.2f} ç§’")

    plt.figure(figsize=(8, 5))
    plt.plot(df_epoch['Epoch'], time_deltas, marker='o', markersize=4, color='#2E8B57', alpha=0.8)
    plt.axhline(y=avg_time, color='#2E8B57', linestyle='--', alpha=0.5, label=f'Avg: {avg_time:.1f}s')
    plt.title('Training Time Cost per Epoch')
    plt.xlabel('Epoch')
    plt.ylabel('Duration (seconds)')
    plt.legend()
    plt.grid(True, linestyle='--', alpha=0.5)
    plt.tight_layout()
    plt.savefig(f'{current_save_dir}/6_time_efficiency.png')
    plt.close()

    # ==========================================
    # [NEW] å›¾ 7: Batch Loss åˆ†å¸ƒ (Boxplot)
    # ==========================================
    # å±•ç¤ºæ¯ä¸ª Epoch çš„ Loss åˆ†å¸ƒï¼Œè§‚å¯Ÿæ”¶æ•›çš„æ–¹å·®å˜åŒ–
    plt.figure(figsize=(10, 6))
    # ä¸ºäº†é˜²æ­¢ Epoch å¤ªå¤šå¯¼è‡´ç®±çº¿å›¾å¤ªæŒ¤ï¼Œæˆ‘ä»¬æ¯éš”å‡ ä¸ª Epoch é‡‡æ ·ä¸€ä¸ªï¼Œæˆ–è€…åªç”»å‰Nå’ŒåN
    # è¿™é‡Œé€‰æ‹©ï¼šå¦‚æœ Epoch < 20 å…¨ç”»ï¼Œå¦åˆ™æ¯éš” (Total/20) ç”»ä¸€ä¸ª
    unique_epochs = df_batch['Epoch'].unique()
    if len(unique_epochs) > 20:
        step = len(unique_epochs) // 20
        selected_epochs = unique_epochs[::step]
    else:
        selected_epochs = unique_epochs
    
    filtered_batch = df_batch[df_batch['Epoch'].isin(selected_epochs)]
    
    # ä¿®å¤ï¼šæ·»åŠ  hue å‚æ•°å’Œ legend=False
    sns.boxplot(x='Epoch', y='Total_Loss', data=filtered_batch, hue='Epoch', palette="Blues", fliersize=1, linewidth=1)
    plt.title('Batch Loss Distribution per Epoch (Variance Analysis)')
    plt.xlabel('Epoch')
    plt.ylabel('Batch Loss')
    plt.grid(axis='y', linestyle='--', alpha=0.5)
    plt.tight_layout()
    plt.savefig(f'{current_save_dir}/7_batch_loss_dist.png')
    plt.close()

    # ==========================================
    # [NEW] å›¾ 8: Loss ä¸ LR è”åˆåˆ†æ (Dual Axis)
    # ==========================================
    fig, ax1_dual = plt.subplots(figsize=(9, 6))
    
    color_loss = '#D76364'
    ax1_dual.set_xlabel('Epoch')
    ax1_dual.set_ylabel('Val Loss', color=color_loss)
    ax1_dual.plot(df_epoch['Epoch'], df_epoch['Val_Loss'], color=color_loss, label='Val Loss', linewidth=2)
    ax1_dual.tick_params(axis='y', labelcolor=color_loss)
    
    ax2_dual = ax1_dual.twinx()  # å®ä¾‹åŒ–ç¬¬äºŒä¸ªè½´
    color_lr = '#6D6D6D'
    ax2_dual.set_ylabel('Learning Rate', color=color_lr)
    ax2_dual.plot(df_epoch['Epoch'], df_epoch['LR'], color=color_lr, linestyle='--', alpha=0.6, label='LR')
    ax2_dual.tick_params(axis='y', labelcolor=color_lr)
    ax2_dual.set_yscale('log')

    plt.title('Validation Loss vs Learning Rate')
    fig.tight_layout()
    plt.savefig(f'{current_save_dir}/8_loss_lr_combined.png')
    plt.close()

    print(f"\nğŸ‰ Plots saved to: {current_save_dir}/")

if __name__ == '__main__':
    import argparse
    import sys
    import glob
    
    # Interactive Menu if no arguments provided
    if len(sys.argv) == 1:
        print("="*60)
        print("ğŸ“Š FADE-Net Plotting Wizard")
        print("="*60)
        
        # Scan for seeds
        logs = glob.glob('training_log_seed*.csv')
        found_seeds = []
        for log in logs:
            try:
                s = log.replace('training_log_seed', '').replace('.csv', '')
                found_seeds.append(s)
            except:
                pass
        
        # Sort seeds: 2026 first, then others numerically
        found_seeds.sort(key=lambda x: (0 if x == '2026' else 1, x))
        
        print("ğŸ” Found the following experiments:")
        menu_map = {}
        
        # Option 1: Auto-Detect (Latest)
        print("   1. [Auto]      Latest Modified Experiment")
        menu_map['1'] = 'AUTO'
        
        # List found seeds
        idx = 2
        for s in found_seeds:
            print(f"   {idx}. [Seed {s}]   Run: training_log_seed{s}.csv")
            menu_map[str(idx)] = s
            idx += 1
            
        # Legacy Option
        if os.path.exists('training_log.csv'):
             print(f"   {idx}. [Legacy]    Standard Log (No Seed)")
             menu_map[str(idx)] = 'LEGACY'
             idx += 1
             
        print(f"   m. [Manual]    Enter Seed ID Manually")
        print("   q. [Quit]      Exit")
        print("-" * 60)
        
        try:
            choice = input(f"ğŸ‘‰ Select experiment to plot [1-{idx-1}]: ").strip().lower()
            
            if choice == 'q':
                print("ğŸ‘‹ Exiting.")
                sys.exit(0)
            elif choice == 'm':
                 manual_seed = input("ğŸ‘‰ Enter Seed ID: ").strip()
                 sys.argv.extend(['--seed', manual_seed])
            elif choice in menu_map:
                selection = menu_map[choice]
                if selection == 'AUTO':
                    # Explicitly find latest log to enforce "Latest" behavior
                    # even if legacy training_log.csv exists
                    if logs:
                        latest_log = max(logs, key=os.path.getmtime)
                        s = latest_log.replace('training_log_seed', '').replace('.csv', '')
                        print(f"ğŸš€ Auto-Selected Latest: Seed {s}")
                        sys.argv.extend(['--seed', s])
                    else:
                        pass # Fallback to load_real_data logic
                elif selection == 'LEGACY':
                    # Check if legacy file actually exists to avoid error
                    if os.path.exists('training_log.csv'):
                        pass # Pass nothing, load_real_data picks legacy
                    else:
                        print("âŒ Legacy log not found.")
                        sys.exit(1)
                else:
                    sys.argv.extend(['--seed', selection])
            else:
                 print("âŒ Invalid choice. Using Auto-Detect.")
                 
        except KeyboardInterrupt:
             sys.exit(0)

    parser = argparse.ArgumentParser()
    parser.add_argument('--seed', type=int, help='Specify seed to plot (e.g. 2026)')
    args = parser.parse_args()
    
    # Special handling for Legacy override:
    # If user explicitly wants Legacy but training_log.csv is missing, it will error.
    # If user wants Auto, args.seed is None.
    
    plot_thesis_suite(seed=args.seed)