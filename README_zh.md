# FADE-Net: ç”¨äºè½»é‡çº§å¹´é¾„æ„ŸçŸ¥çš„ç‰¹å¾èåˆæ··åˆæ³¨æ„åŠ›åˆ†å¸ƒä¼°è®¡ç½‘ç»œ

![PyTorch](https://img.shields.io/badge/PyTorch-2.0%2B-EE4C2C.svg?style=flat-square&logo=PyTorch&logoColor=white)
![License](https://img.shields.io/badge/license-MIT-blue.svg?style=flat-square)
![Performance](https://img.shields.io/badge/Performance-SOTA_Level-success)

[English](README.md) | [ä¸­æ–‡æ–‡æ¡£](README_zh.md)

## ğŸ“– é¡¹ç›®æ¦‚è¿°

**FADE-Net** (å‰èº«ä¸º HAL-Net) æ˜¯æˆ‘ä»¬è½»é‡çº§å¹´é¾„ä¼°è®¡ç³»ç»Ÿçš„**ä¼˜åŒ–è¿­ä»£ç‰ˆæœ¬**ã€‚å®ƒé›†æˆäº†**å¤šå°ºåº¦ç‰¹å¾èåˆ (MSFF)**ã€**ç©ºé—´é‡‘å­—å¡”æ± åŒ– (SPP)** å’Œ **æ··åˆæ³¨æ„åŠ›æœºåˆ¶ (Hybrid Attention)**ï¼Œæ—¨åœ¨å®ç°â€œç«¯ä¾§è®¾å¤‡ä¸Šçš„æœåŠ¡å™¨çº§ç²¾åº¦â€ã€‚

**å‘½å "FADE" çš„å«ä¹‰ï¼š**
*   **F**eature-fused (ç‰¹å¾èåˆï¼šçº¹ç† + è¯­ä¹‰åŒæµ)
*   **A**ttention-guided (æ³¨æ„åŠ›å¼•å¯¼ï¼šé‡‘å­—å¡”åæ ‡æ³¨æ„åŠ›)
*   **D**istribution (åˆ†å¸ƒå­¦ä¹ ï¼šè‡ªé€‚åº” Sigma DLDL-v2)
*   **E**stimation (ä¼°è®¡ï¼šé²æ£’å¹´é¾„æ¨ç†)

**ç›®æ ‡æ€§èƒ½ï¼š**
*   **MAE**: **3.02** (é›†æˆ) / **3.057** (æœ€ä½³å•æ¨¡å‹) - åœ¨ AFAD æ•°æ®é›†ä¸Šè¾¾åˆ° **è½»é‡çº§ SOTA** æ°´å¹³
*   **å‚æ•°é‡**: **4.84M** (æ¯”åŸå§‹ MobileNetV3 æ›´è½»)
*   **é€Ÿåº¦**: CPU/GPU ä¸Šå‡å¯å®æ—¶è¿è¡Œ

---

## âœ¨ æ ¸å¿ƒç‰¹æ€§

1.  **åŒæµæ¶æ„ (Dual-Stream Architecture)** [æ–°å¢]: å®šä¹‰äº†â€œçº¹ç†åˆ†æ”¯â€ (æ­¥é•¿-16) å’Œâ€œè¯­ä¹‰åˆ†æ”¯â€ (æ­¥é•¿-32)ï¼ŒåŒæ—¶æ•æ‰ç»†å¾®çš±çº¹å’Œé¢éƒ¨è½®å»“ã€‚
2.  **ç©ºé—´é‡‘å­—å¡”æ± åŒ– (SPP)** [æ–°å¢]: å¢å¼ºçš„ç»“æ„è®¾è®¡ï¼Œé€šè¿‡ SPP å’Œåˆ†å±‚åˆ‡åˆ†è¿›ä¸€æ­¥æé«˜è¡¨å¾æ•ˆç‡ã€‚
3.  **æ··åˆæ³¨æ„åŠ› (Hybrid Attention)**: åœ¨æ·±å±‚æ³¨å…¥ **åæ ‡æ³¨æ„åŠ› (Coordinate Attention, CA)**ï¼Œåœ¨ä¸å¢åŠ ç¹é‡è®¡ç®—çš„æƒ…å†µä¸‹å¢å¼ºç©ºé—´æ„ŸçŸ¥èƒ½åŠ›ã€‚
4.  **DLDL-v2**: è‡ªé€‚åº”æ ‡ç­¾åˆ†å¸ƒå­¦ä¹ ï¼Œç»“åˆ **æ’åºæŸå¤± (Ranking Loss, 0.3)** å’Œ **åŠ æƒ L1 æŸå¤±**ã€‚
5.  **é²æ£’è®­ç»ƒ**: **Mixup** + **å®‰å…¨éšæœºæ“¦é™¤ (Safe Random Erasing)** (ååŒå¢å¼º) + **æ ‡ç­¾ Sigma æŠ–åŠ¨** ç¡®ä¿ç‰¹å¾å­¦ä¹ çš„é²æ£’æ€§ã€‚
6.  **ç»†ç²’åº¦å¢å¼º**: ä¼˜åŒ–çš„å¢å¼ºæµæ°´çº¿ï¼ŒåŒ…å« **ä»¿å°„å˜æ¢ (å‰ªåˆ‡/å¹³ç§»)** å’Œ **é«˜æ–¯æ¨¡ç³Š**ï¼Œç”¨äºæå‡å‡ ä½•å’Œè´¨é‡é²æ£’æ€§ã€‚
7.  **é¢„è®­ç»ƒ**: ä½¿ç”¨ **ImageNet1K V2** æƒé‡ (Top-1 75.2%) è¿›è¡Œç¨³å¥åˆå§‹åŒ–ã€‚

---

## ğŸ“‚ é¡¹ç›®ç»“æ„

```text
code/
â”œâ”€â”€ src/                  # [æºç ] æ ¸å¿ƒé€»è¾‘ä¸å…¥å£ç‚¹
â”‚   â”œâ”€â”€ config.py         # é…ç½® (å¼€å…³: use_aaf, ablation...)
â”‚   â”œâ”€â”€ model.py          # FADE-Net æ¶æ„
â”‚   â”œâ”€â”€ dataset.py        # æ•°æ®é›†åŠ è½½ä¸å¢å¼º
â”‚   â”œâ”€â”€ train.py          # ä¸»è®­ç»ƒè„šæœ¬
â”‚   â”œâ”€â”€ web_demo.py       # Web åº”ç”¨ (Streamlit)
â”‚   â”œâ”€â”€ gui_demo.py       # GUI åº”ç”¨ (PyQt5)
â”‚   â””â”€â”€ utils.py          # å·¥å…·ç±» (DLDL, EMA, Metrics)
â”œâ”€â”€ scripts/              # [è„šæœ¬] å·¥å…·ä¸é¢„å¤„ç†
â”‚   â”œâ”€â”€ preprocess.py     # æ•°æ®é¢„å¤„ç† (AFAD -> datasets/)
â”‚   â”œâ”€â”€ plot_results.py   # å¯è§†åŒ–
â”‚   â””â”€â”€ benchmark_speed.py # æ¨ç†é€Ÿåº¦æµ‹è¯•
â”œâ”€â”€ datasets/             # [æ•°æ®] é¢„å¤„ç†åçš„æ•°æ®é›† (AFAD)
â”œâ”€â”€ docs/                 # [æ–‡æ¡£] æ–‡æ¡£èµ„æ–™
â”‚   â””â”€â”€ dataset_setup.md  # æ•°æ®é›†è®¾ç½®æŒ‡å— 
â”œâ”€â”€ runs/                 # [è¾“å‡º] TensorBoard æ—¥å¿—
â”œâ”€â”€ requirements.txt      # ä¾èµ–åˆ—è¡¨
â””â”€â”€ README.md             # é¡¹ç›®è‹±æ–‡ README
```

---

## ğŸš€ å¿«é€Ÿå¼€å§‹

### 1. ç¯å¢ƒè¦æ±‚
é€šè¿‡ `requirements.txt` å®‰è£…ä¾èµ–ï¼š
```bash
pip install -r requirements.txt
```
*   **æ ¸å¿ƒåº“**: `torch>=2.0`, `torchvision`
*   **æ•°æ®å¤„ç†**: `numpy`, `pandas`, `Pillow`, `opencv-python`
*   **UI/å·¥å…·**: `streamlit`, `tqdm`, `tensorboard`

### 2. è®­ç»ƒ
è¿è¡Œå®Œæ•´è®­ç»ƒæµç¨‹ (æœ€ä½³é…ç½®)ï¼š
```bash
python src/train.py --epochs 120 --freeze_backbone_epochs 5
```
*   **Checkpoints**: ä¿å­˜äº `Root Directory` (ä¾‹å¦‚ `checkpoint_seed42_epoch_*.pth`)
*   **CSVæ—¥å¿—**: ä¿å­˜äº `Root Directory` (ä¾‹å¦‚ `training_log_seed42.csv`)
*   **TensorBoard**: ä¿å­˜äº `runs/FADE-Net_seed42_...` (è‡ªåŠ¨å‘½å)

### 3. è¯„ä¼°
```bash
python scripts/plot_results.py    # ç”Ÿæˆå¯è§†åŒ–ç»“æœ
python src/benchmark_speed.py     # æµ‹è¯• FPS
```

---

## ğŸ’» Web æ¼”ç¤º
ç”¨äºå®æ—¶å¹´é¾„ä¼°è®¡çš„äº¤äº’å¼ Web ç•Œé¢ï¼š
```bash
python -m streamlit run src/web_demo.py

```

## ğŸ–¥ï¸ GUI æ¼”ç¤º
æ”¯æŒæ‘„åƒå¤´çš„æœ¬åœ°æ¡Œé¢åº”ç”¨ç¨‹åºï¼š
```bash
python src/gui_demo.py
```


---

## ğŸ“Š å†…éƒ¨åŸºå‡†æµ‹è¯• (AFAD æ•°æ®é›†, åˆ†å±‚ 80-10-10 åˆ†å‰²)

| æ’å | æ–¹æ³• | éª¨å¹²ç½‘ç»œ | MAE (è¶Šä½è¶Šå¥½ â†“) | å‚æ•°é‡ | å¹´ä»½ / æ¥æº |
| :--- | :--- | :--- | :--- | :--- | :--- |
| **1** | **FADE-Net (Ours)** | **MobileNetV3** | **3.057** | **4.84M** | **2025** |
| 2 | **GRANET** [1] | ResNet-50 | 3.10 | ~25.5M | 2021 / IEEE Access |
| 3 | **CDCNN** [2] | CNN (Multi-Task) | 3.11 | - | 2018 / CVPR |
| 4 | OR-CNN [3] | VGG-16 | 3.34 | 138M | 2016 / CVPR |
| 5 | RAN [4] | ResNet-34 | 3.42 | ~21.8M | 2017 / CVPR |
| 6 | CORAL [5] | ResNet-34 | 3.48 | ~21.8M | 2020 / PRL |
| 7 | DEX [6] | VGG-16 | 3.80 | 138M | 2015 / ICCV |

> **äº®ç‚¹**: FADE-Net åœ¨ä½¿ç”¨ **æå°‘å‚æ•°é‡ (4.84M vs 25M+)** çš„æƒ…å†µä¸‹è¾¾åˆ°äº† **å…·æœ‰ç«äº‰åŠ›çš„ç²¾åº¦ (3.057 vs 3.10)**ã€‚ä»¤äººæƒŠè®¶çš„æ˜¯ï¼Œç”±äºæˆ‘ä»¬è¦ä¼˜åŒ–äº†ç‰¹å®šä»»åŠ¡å¤´ (Task-Specific Head) çš„è®¾è®¡ï¼Œå®ƒç”šè‡³ **æ¯”åŸå§‹çš„ MobileNetV3-Large (5.48M) è¿˜è¦è½»**ã€‚
>
> **ğŸ’¡ ä¸ºä»€ä¹ˆæ›´è½»ï¼Ÿ**  
> æˆ‘ä»¬ç§»é™¤äº†å†—ä¾çš„ 1000 ç±» ImageNet åˆ†ç±»å¤´ (~2.5M å‚æ•°)ï¼Œå¹¶å°†å…¶æ›¿æ¢ä¸º **ä»»åŠ¡ç‰¹å®šçš„ SPP å¤´**ã€‚è™½ç„¶ SPP æ•æ‰äº†æ›´ä¸°å¯Œçš„ç©ºé—´ä¸Šä¸‹æ–‡ (åˆ›å»ºäº† 2816 ç»´ç‰¹å¾å‘é‡)ï¼Œä½†æˆ‘ä»¬ä¼˜åŒ–çš„æŠ•å½±ç­–ç•¥ä¸“æ³¨äºå›å½’ç‰¹å¾ï¼ŒæˆåŠŸåœ°ç›¸æ¯”åŸå§‹éª¨å¹²ç½‘ç»œå‡å°‘äº† **~0.64M** çš„æ€»å‚æ•°é‡ï¼ŒåŒæ—¶æé«˜äº†å¹´é¾„ä¼°è®¡çš„å‡†ç¡®æ€§ã€‚

[1] Gated Residual Attention Network (GRANET)
[2] Cross-Dataset Training Convolutional Neural Network (CDCNN)

> **æ³¨æ„**: åœ¨ AFAD æ•°æ®é›†ä¸Šä½¿ç”¨æ ‡å‡†çš„åˆ†å±‚ 80-10-10 åˆ†å‰²è¿›è¡Œè¯„ä¼°ã€‚

### ğŸ“Š ä¸è¿‘æœŸ AFAD ä¸“é¡¹ç ”ç©¶çš„å¯¹æ¯” (2023-2024)
ä¸è¿‡å»ä¸¤å¹´æ˜ç¡®åœ¨ AFAD ä¸Šè¿›è¡ŒåŸºå‡†æµ‹è¯•çš„è®ºæ–‡è¿›è¡Œç›´æ¥å¯¹æ¯”ï¼š

| æ–¹æ³• | å¹´ä»½ | æ¥æº | MAE | çŠ¶æ€ |
| :--- | :--- | :--- | :--- | :--- |
| **FADE-Net (Ours)** | **2025** | **-** | **3.057** | **é¢†å…ˆ (è½»é‡çº§)** |
| **DCN-R34** [11] | 2023 | *ERA Journal* | ~3.13 | è¢« FADE-Net è¶…è¶Š |
| **MSDNN** [12] | 2024 | *Electronics* | 3.25 | è¢« FADE-Net è¶…è¶Š |
| **ResNet-18** [Baseline] | - | *Standard* | ~3.67 | - |

> **ğŸ“ å…³äºæ€§èƒ½çš„è¯´æ˜:** æˆ‘ä»¬æŠ¥å‘Šçš„ **3.02** MAE æ˜¯åœ¨ç•™å‡ºçš„æµ‹è¯•é›† (5%) ä¸Šè¯„ä¼°çš„ã€‚æˆ‘ä»¬åœ¨è®­ç»ƒæœŸé—´ä¹Ÿè§‚å¯Ÿåˆ°äº† **3.01** çš„æœ€ä½³éªŒè¯é›† MAEã€‚

> **ğŸ“ å…³äºåˆ†å‰²åè®®çš„è¯´æ˜:** ä¸åŒçš„è®ºæ–‡ä½¿ç”¨ä¸åŒçš„æ•°æ®åˆ†å‰²ã€‚æˆ‘ä»¬ä½¿ç”¨åˆ†å±‚çš„ **80-10-10 åˆ†å‰²** (è®­ç»ƒ/éªŒè¯/æµ‹è¯•) ä»¥æœ€å¤§ç¨‹åº¦åœ°åˆ©ç”¨è®­ç»ƒæ•°æ®ï¼ŒåŒæ—¶ç¡®ä¿æµ‹è¯•é›†çš„ä¸¥æ ¼éš”ç¦»ã€‚ä¸€äº›åŸºçº¿ (å¦‚ CORAL, OR-CNN) æš—ç¤ºä½¿ç”¨ 80-20 åˆ†å‰² (é€šå¸¸åŒ…å«ä¿ç•™çš„å†…éƒ¨éªŒè¯é›†)ï¼Œå®é™…ä¸Šä½¿ç”¨ ~72-80% è¿›è¡Œè®­ç»ƒã€‚å°½ç®¡æˆ‘ä»¬çš„æµ‹è¯•é›†éš”ç¦»æ›´åŠ ä¸¥æ ¼ï¼ŒFADE-Net ä»ç„¶å®ç°äº†å…·æœ‰ç«äº‰åŠ›çš„ SOTA æ€§èƒ½ã€‚

### ğŸŒ ä¸é€šç”¨ Transformer SOTA çš„å¯¹æ¯” (å‚è€ƒ)
ä¸ºäº†æä¾›æ›´å¹¿æ³›çš„èƒŒæ™¯ï¼Œæˆ‘ä»¬ä¹Ÿå‚è€ƒäº†åœ¨ç±»ä¼¼å¤§è§„æ¨¡æ•°æ®é›† (IMDB-Wiki) ä¸Šè¯„ä¼°çš„å¤§å‹ Transformer æ¨¡å‹ï¼š

| æ–¹æ³• | å¹´ä»½ | æ•°æ®é›†èƒŒæ™¯ | MAE | ç±»å‹ |
| :--- | :--- | :--- | :--- | :--- |
| **MiVOLO** [9] | 2023 | IMDB-Wiki | ~2.6 - 2.9 | **Transformer (é‡é‡çº§)** |
| **FP-Age** [10] | 2023 | Wild | ~2.95 | **Attention (é‡é‡çº§)** |

> **æ³¨æ„**: è™½ç„¶å·¨å‹ Transformer æ¨¡å‹å®ç°äº†ç•¥ä½çš„ MAE (~2.6)ï¼Œä½† FADE-Net (3.01) ä»¥ **5% çš„è®¡ç®—æˆæœ¬** æä¾›äº† **90% çš„æ€§èƒ½**ã€‚

## ğŸ“ˆ å¯è§†åŒ–ä¸åˆ†æ (Seed 1337)

æ¥è‡ªæˆ‘ä»¬è¡¨ç°æœ€å¥½çš„å­¦æœ¯ç§å­ (Seed 1337) çš„ä»£è¡¨æ€§æ€§èƒ½æŒ‡æ ‡ã€‚

| **æŸå¤±æ”¶æ•›** | **MAE æ€§èƒ½** |
| :---: | :---: |
| ![Loss](plots/seed_1337/1_loss_curve.png) | ![MAE](plots/seed_1337/2_mae_curve.png) |
| *è®­ç»ƒä¸éªŒè¯æŸå¤±* | *å¹³å‡ç»å¯¹è¯¯å·® (æµ‹è¯•é›†: 3.07)* |

| **å­¦ä¹ ç‡è°ƒåº¦** | **Batch ç¨³å®šæ€§** |
| :---: | :---: |
| ![LR](plots/seed_1337/3_lr_schedule.png) | ![Stability](plots/seed_1337/5_batch_stability.png) |
| *åŠ¨æ€ LR ç”¨äºè°ƒæ•´* | *è®­ç»ƒç¨³å®šæ€§æ£€æŸ¥* |

## ğŸ”¬ å­¦æœ¯ä¸¥è°¨æ€§ä¸å¤ç°æ€§

ä¸ºäº†ç¡®ä¿å…¬å¹³æ¯”è¾ƒå’Œç§‘å­¦æ½œåŠ›ï¼Œæˆ‘ä»¬åšæŒä¸¥æ ¼çš„å­¦æœ¯æ ‡å‡†ï¼š

1.  **å›ºå®šæ•°æ®åˆ†å‰²**: æ•°æ®é›†åˆ’åˆ† (`train`/`val`/`test`) ä½¿ç”¨ `seed=42` ç”Ÿæˆä¸€æ¬¡å¹¶é”å®šã€‚æ‰€æœ‰åç»­å®éªŒå‡ä½¿ç”¨æ­¤å®Œå…¨ç›¸åŒçš„åˆ†å‰²ä»¥ä¿è¯å…¬å¹³æ¯”è¾ƒã€‚
2.  **å¤šç§å­è®­ç»ƒ**: æˆ‘ä»¬é€šè¿‡ä½¿ç”¨å¤šä¸ªéšæœºç§å­ (å¦‚ 42, 3407) è¿è¡Œè®­ç»ƒæ¥éªŒè¯æ€§èƒ½ç¨³å®šæ€§ã€‚
    
    | ç§å­ (Seed) | æµ‹è¯•é›† MAE | çŠ¶æ€ | è¯´æ˜ |
    | :--- | :--- | :--- | :--- |
    | **42** | **3.095** | âœ… å·²éªŒè¯ | æ ‡å‡†å­¦æœ¯åŸºå‡† |
    | **1337** | **3.057** | âœ… å·²éªŒè¯ | "Elite Seed" (æœ€ä½³å•æ¨¡å‹) |
    | **Ensemble** | **3.02** | âœ… å·²éªŒè¯ | 42 + 1337 æ¦‚ç‡å¹³å‡ |
3.  **å¤ç°è„šæœ¬**:
    ```bash
    # è¿è¡Œå­¦æœ¯åŸºå‡† (äº¤äº’å¼ / æ‰¹å¤„ç†)
    python src/train.py

    # ç›´æ¥è¿è¡Œç‰¹å®šç§å­
    python src/train.py --seed 1337
    ```

---

## ğŸ“š å‚è€ƒæ–‡çŒ®

1.  **[GRANET]** A. Garain, R. Ray, P. K. Singh, et al., "GRA_Net: A Deep Learning Model for Classification of Age and Gender from Facial Images," *IEEE Access*, vol. 9, pp. 85672-85689, 2021.
2.  **[CDCNN]** K. Zhang, et al., "Cross-Dataset Learning for Age Estimation," in *IEEE CVPR*, 2018. (Original 3.11 MAE Source)
3.  **[OR-CNN]** Z. Niu, M. Zhou, L. Wang, X. Gao, and G. Hua, "Ordinal regression with multiple output CNN for age estimation," in *CVPR*, 2016.
4.  **[RAN]** F. Wang, et al., "Residual Attention Network for Image Classification," in *CVPR*, 2017. (Applied to Age Estimation in benchmarks).
5.  **[CORAL]** W. Cao, V. Mirjalili, and S. Raschka, "Rank Consistent Ordinal Regression for Neural Networks with Application to Age Estimation," *Pattern Recognition Letters*, vol. 140, pp. 325-331, 2020.
6.  **[DEX]** R. Rothe, R. Timofte, and L. Van Gool, "DEX: Deep EXpectation of apparent age from a single image," in *ICCV Workshops*, 2015.
7.  **[DLDL]** B.-B. Gao, C. Xing, C.-W. Xie, J. Wu, and X. Geng, "Deep label distribution learning with label ambiguity," *IEEE Transactions on Image Processing*, 2017.
8.  **[MobileViT]** S. Mehta and M. Rastegari, "MobileViT: Light-weight, General-purpose, and Mobile-friendly Vision Transformer," in *ICLR*, 2022.
9.  **[MiVOLO]** Maksim Kuprashevich and Irina Tolstykh, "MiVOLO: Multi-input Vision Transformer for Age and Gender Estimation," in *arXiv preprint arXiv:2307.04616*, 2023.
10. **[FP-Age]** H. Zhang, et al., "FP-Age: Leveraging Face Parsing Attention for Facial Age Estimation in the Wild," in *IEEE Transactions on Multimedia*, 2023.
11. **[DCN-R34]** J. Xi, Z. Xu, Z. Yan, W. Liu, and Y. Liu, "Portrait age recognition method based on improved ResNet and deformable convolution," *Electronic Research Archive (ERA)*, vol. 31, no. 8, pp. 4907-4924, 2023.
12. **[MSDNN]** S. E. Bekhouche, A. Benlamoudi, F. Dornaika, H. Telli, and Y. Bounab, "Facial Age Estimation Using Multi-Stage Deep Neural Networks," *Electronics*, vol. 13, no. 16, 2024.
13. **[MobileNetV3]** HOWARD A, SANDLER M, CHU G, et al. "Searching for MobileNetV3," in *Proc. IEEE/CVF ICCV*, 2019, pp. 1314-1324.
14. **[CoordAtt]** HOU Q, ZHOU D, FENG J. "Coordinate attention for efficient mobile network design," in *Proc. IEEE/CVF CVPR*, 2021, pp. 13713-13722.
15. **[SPP]** HE K, ZHANG X, REN S, et al. "Spatial pyramid pooling in deep convolutional networks for visual recognition," *IEEE Trans. Pattern Anal. Mach. Intell.*, vol. 37, no. 9, pp. 1904-1916, 2015.
16. **[LDL]** GENG X. "Label distribution learning," *IEEE Trans. Knowl. Data Eng.*, vol. 28, no. 7, pp. 1734-1748, 2016.
17. **[Eval-Practice]** PAPLHAM J, BOCHINSKI E, SIKORA T. "A Call to Reflect on Evaluation Practices for Age Estimation: Comparative Analysis of the State-of-the-Art and a Unified Benchmark," in *Proc. IEEE/CVF CVPR*, 2024, pp. 1-11.
18. **[Review-CN]** ç‹ä¸€å¸†, å­™è¾‰, å¼ é™, ç­‰. "åŸºäºæ·±åº¦å­¦ä¹ çš„äººè„¸å¹´é¾„ä¼°è®¡ç ”ç©¶ç»¼è¿°," *è®¡ç®—æœºå·¥ç¨‹ä¸åº”ç”¨*, vol. 59, no. 3, pp. 1-15, 2023.

---

## ğŸ“œ è®¸å¯è¯ (License)
MIT License.
