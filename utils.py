# utils.py
import torch
import torch.nn as nn
import torch.nn.functional as F
import numpy as np
import cv2
# import mediapipe as mp
import copy
import random
import os

# ==========================================
# 0. Reproducibility Tool
# ==========================================
def seed_everything(seed=42):
    """
    Fix random seeds for reproducibility.
    """
    random.seed(seed)
    os.environ['PYTHONHASHSEED'] = str(seed)
    np.random.seed(seed)
    torch.manual_seed(seed)
    torch.cuda.manual_seed(seed)
    torch.cuda.manual_seed_all(seed)
    torch.backends.cudnn.deterministic = True
    torch.backends.cudnn.benchmark = False
    print(f"ğŸŒ± Global Seed Set to {seed}")

# ==========================================
# 1. DLDL æ ¸å¿ƒå¤„ç†ç±»
# ==========================================
class DLDLProcessor:
    def __init__(self, config):
        self.max_age = config.max_age
        self.num_classes = config.num_classes
        
        # åŠ¨æ€ sigma å‚æ•°
        self.use_dldl_v2 = getattr(config, 'use_dldl_v2', True)
        
        # Original logic: use_adaptive_sigma was a separate flag.
        # Now we merge it into 'use_dldl_v2' for ablation simplicity,
        # OR we keep it independent but default it to True if dldl_v2 is True.
        
        # Let's say: use_adaptive_sigma is active ONLY IF use_dldl_v2 is True.
        self.use_adaptive_sigma = self.use_dldl_v2 and getattr(config, 'use_adaptive_sigma', False)
        
        if self.use_adaptive_sigma:
            self.sigma_min = getattr(config, 'sigma_min', 1.5)
            self.sigma_max = getattr(config, 'sigma_max', 3.5)
            # print("âœ… [Loss] Adaptive Sigma: ENABLED") # avoid spamming
        else:
            self.sigma = config.sigma
            # print(f"â„¹ï¸ [Loss] Adaptive Sigma: DISABLED (Fixed sigma={self.sigma})")
        
        # Label Smoothing å‚æ•°
        self.label_smoothing = getattr(config, 'label_smoothing', 0.0)
        
        # é¢„å…ˆç”Ÿæˆå¹´é¾„ç´¢å¼•å¼ é‡ [0, 1, ..., num_classes-1]
        self.age_indices = torch.arange(0, config.num_classes, dtype=torch.float32)

    def generate_label_distribution(self, age_scalar):
        """
        å°†æ ‡é‡å¹´é¾„è½¬åŒ–ä¸ºç¦»æ•£çš„é«˜æ–¯æ¦‚ç‡åˆ†å¸ƒã€‚
        æ”¹è¿›:
        1. åŠ¨æ€ sigma: å¹´é¾„è¶Šå¤§,ä¸ç¡®å®šæ€§è¶Šé«˜
        2. Label Smoothing: å¹³æ»‘åˆ†å¸ƒ,é˜²æ­¢è¿‡æ‹Ÿåˆ
        """
        # 2024-12-16: Convert numpy scalar to tensor to prevent warning
        if not isinstance(age_scalar, torch.Tensor):
            age_scalar = torch.tensor(age_scalar, dtype=torch.float32)

        # åŠ¨æ€è®¡ç®— sigma (å¹´é¾„è¶Šå¤§,sigma è¶Šå¤§)
        if self.use_adaptive_sigma:
            sigma = self.sigma_min + (self.sigma_max - self.sigma_min) * (age_scalar / self.max_age)
        else:
            sigma = self.sigma

        # è®¡ç®—æ¯ä¸ªå¹´é¾„èŠ‚ç‚¹ j ä¸ çœŸå®å¹´é¾„ y çš„å·®å¼‚
        # $$P(j|x) \propto e^{-\frac{(j-y)^2}{2\sigma^2}}$$
        diff = self.age_indices - age_scalar
        prob_dist = torch.exp(-0.5 * (diff / sigma) ** 2)

        # å½’ä¸€åŒ–ä¿è¯æ¦‚ç‡å’Œä¸º1
        prob_dist = prob_dist / torch.sum(prob_dist)
        
        # Label Smoothing: æ··åˆå‡åŒ€åˆ†å¸ƒ
        if self.label_smoothing > 0:
            uniform_dist = torch.ones_like(prob_dist) / self.num_classes
            prob_dist = (1 - self.label_smoothing) * prob_dist + self.label_smoothing * uniform_dist

        return prob_dist

    def expectation_regression(self, predicted_probs):
        """
        æ¨ç†ç«¯ï¼šå¯¹é¢„æµ‹åˆ†å¸ƒè¿›è¡ŒåŠ æƒæ±‚å’Œã€‚
        """
        if predicted_probs.device != self.age_indices.device:
            self.age_indices = self.age_indices.to(predicted_probs.device)

        # æœŸæœ›è®¡ç®—ï¼šæ¦‚ç‡ * å¹´é¾„å€¼ çš„æ€»å’Œ
        batch_expected_age = torch.sum(predicted_probs * self.age_indices, dim=1)
        return batch_expected_age

# ==========================================
# 2. äººè„¸å¯¹é½å·¥å…· (Face Aligner)
# ==========================================
class FaceAligner:
    def __init__(self):
        import mediapipe as mp
        self.mp_face_detection = mp.solutions.face_detection
        self.detector = self.mp_face_detection.FaceDetection(min_detection_confidence=0.3)

    def align(self, image, desired_size=224, desired_left_eye=(0.32, 0.35)):
        """
        æ ‡å‡†äººè„¸å¯¹é½ (Similarity Transform)
        1. æ—‹è½¬: çœ¼ç›è¿çº¿æ°´å¹³
        2. ç¼©æ”¾+å¹³ç§»: å°†çœ¼ç›å›ºå®šåœ¨å›¾ç‰‡çš„ç‰¹å®šä½ç½®
        Output: 224x224 (é»˜è®¤) çš„æ ‡å‡†äººè„¸å›¾
        """
        try:
            img_np = np.array(image)
            h, w, c = img_np.shape
            
            # æ£€æµ‹äººè„¸
            results = self.detector.process(img_np)
            
            if not results.detections:
                return None # æ£€æµ‹å¤±è´¥ï¼Œç›´æ¥ä¸¢å¼ƒ
            
            # å–ç½®ä¿¡åº¦æœ€é«˜çš„ä¸€ä¸ª
            detection = results.detections[0]
            # score = detection.score[0]
            # if score < 0.5: # å†æ¬¡è¿‡æ»¤ä½ç½®ä¿¡åº¦ -> å·²ç”± init å‚æ•°æ§åˆ¶ï¼Œæ­¤å¤„ç§»é™¤ä»¥é¿å…çŸ›ç›¾
            #    return None
                
            keypoints = detection.location_data.relative_keypoints
            # MediaPipe: 0=å³çœ¼(å›¾å·¦), 1=å·¦çœ¼(å›¾å³)
            right_eye = np.array([keypoints[0].x * w, keypoints[0].y * h])
            left_eye = np.array([keypoints[1].x * w, keypoints[1].y * h])
            
            # 1. è®¡ç®—è§’åº¦ (ä½¿å¾— å·¦çœ¼-å³çœ¼ è¿çº¿æ°´å¹³)
            # dy = right - left (yè½´å‘ä¸‹)
            dY = right_eye[1] - left_eye[1]
            dX = right_eye[0] - left_eye[0]
            angle = np.degrees(np.arctan2(dY, dX)) - 180
            
            # 2. è®¡ç®—å½“å‰ç³è·
            dist = np.sqrt((dX ** 2) + (dY ** 2))
            
            # 3. è®¡ç®—ç›®æ ‡ç³è·
            # desired_left_eye[0] æ˜¯å·¦çœ¼(å›¾å³)çš„ x æ¯”ä¾‹ï¼Ÿ 
            # é€šå¸¸ convention: desired_left_eye=(0.35, 0.35) æŒ‡çš„æ˜¯ "Left Eye" (subject's left, image right) çš„ä½ç½®ï¼Ÿ
            # ä¸ï¼Œé€šå¸¸æŒ‡çš„æ˜¯ "Image Left" é‚£ä¸ªçœ¼ç› (å³ Right Eye of subject) åœ¨ 0.35ã€‚
            # è®©æˆ‘ä»¬æ˜ç¡®å®šä¹‰ï¼š
            # æˆ‘ä»¬å¸Œæœ›ï¼šSubject's Right Eye (Image Left) at x = 1.0 - desired_left_eye_x ? No.
            # è®©æˆ‘ä»¬ç”¨æ ‡å‡†å¸¸ç”¨å€¼ï¼š
            # Subject's Right Eye (Image Left) -> (desired_dist_x, desired_dist_y)
            # Subject's Left Eye (Image Right) -> (1-desired_dist_x, desired_dist_y)
            
            desired_right_eye_x = 1.0 - desired_left_eye[0] # e.g. 1 - 0.32 = 0.68
            
            # ç›®æ ‡ç³è· (åƒç´ )
            desired_dist_pix = (desired_right_eye_x - desired_left_eye[0]) * desired_size
            
            # 4. è®¡ç®—ç¼©æ”¾æ¯”ä¾‹
            scale = desired_dist_pix / dist
            
            # 5. è®¡ç®—æ—‹è½¬+ç¼©æ”¾çŸ©é˜µ
            # æ—‹è½¬ä¸­å¿ƒ: ä¸¤çœ¼ä¸­å¿ƒ
            eyes_center = ((left_eye[0] + right_eye[0]) // 2, (left_eye[1] + right_eye[1]) // 2)
            M = cv2.getRotationMatrix2D(eyes_center, angle, scale)
            
            # 6. åŠ å…¥å¹³ç§»åˆ†é‡ (Update Translation)
            # æˆ‘ä»¬å¸Œæœ› eyes_center ç§»åŠ¨åˆ°å›¾ç‰‡ä¸­å¿ƒ (desired_size*0.5, desired_size*desired_eye_y)
            tX = desired_size * 0.5
            tY = desired_size * desired_left_eye[1]
            
            M[0, 2] += (tX - eyes_center[0])
            M[1, 2] += (tY - eyes_center[1])
            
            # 7. Warp
            aligned_np = cv2.warpAffine(img_np, M, (desired_size, desired_size), flags=cv2.INTER_CUBIC)
            
            return Image.fromarray(aligned_np)

        except Exception as e:
            # print(f"Align Fail: {e}")
            return None

# ==========================================
# 3. EMA (æŒ‡æ•°æ»‘åŠ¨å¹³å‡)
# ==========================================
class EMAModel:
    def __init__(self, model, decay=0.999):
        self.model = model
        self.decay = decay
        self.shadow = {}
        self.backup = {}
        self.register()

    def register(self):
        for name, param in self.model.named_parameters():
            if param.requires_grad:
                self.shadow[name] = param.data.clone()

    def update(self):
        for name, param in self.model.named_parameters():
            if param.requires_grad:
                assert name in self.shadow
                new_average = (1.0 - self.decay) * param.data + self.decay * self.shadow[name]
                self.shadow[name] = new_average.clone()

    def apply_shadow(self):
        for name, param in self.model.named_parameters():
            if param.requires_grad:
                self.backup[name] = param.data
                param.data = self.shadow[name]

    def restore(self):
        for name, param in self.model.named_parameters():
            if param.requires_grad:
                assert name in self.backup
                param.data = self.backup[name]
        self.backup = {}

# ==========================================
# 4. é«˜çº§æŸå¤±å‡½æ•° (Ranking Loss)
# ==========================================
class OrderRegressionLoss(nn.Module):
    """
    æ’åºæŸå¤± (Ordinal Regression / Ranking Loss)
    è¿«ä½¿æ¨¡å‹å­¦ä¹  'å¹´é¾„ A > å¹´é¾„ B' è¿™ç§åºå…³ç³»ã€‚
    Ref: "Rank consistent ordinal regression for neural networks with application to age estimation" (CVPR 2016)
    æˆ–è€…ç®€åŒ–ç‰ˆ: å¯¹ logits æ–½åŠ çº¦æŸ or å¯¹ probabilities æ–½åŠ  Rank Loss
    
    è¿™é‡Œé‡‡ç”¨ä¸€ç§ç®€å•ä¸”æœ‰æ•ˆçš„ç­–ç•¥:
    Soft Ranking Loss on Expectation (å€Ÿé‰´ Mean-Variance Loss æ€æƒ³)
    æˆ–è€…å¯¹ Logits è¿›è¡Œ Pairwise çº¦æŸ (è€—æ—¶)
    
    è€ƒè™‘åˆ°è®¡ç®—æ•ˆç‡ï¼Œæˆ‘ä»¬è¿™é‡Œä½¿ç”¨:
    Binary Ordinal Classification Logic çš„å˜ä½“
    è®©æ¨¡å‹è¾“å‡ºä¸ä»…ä»…æ˜¯åˆ†ç±»ï¼Œè¿˜è¦æ»¡è¶³ Ordinal çº¦æŸã€‚
    
    Given logits (N, K), we want:
    if y = 3, then P(y>0)=1, P(y>1)=1, P(y>2)=1, P(y>3)=0...
    """
    def __init__(self, config):
        super(OrderRegressionLoss, self).__init__()
        self.num_classes = config.num_classes
        # Register buffer to avoid device issues
        self.register_buffer('rank_indices', torch.arange(self.num_classes).float().unsqueeze(0)) # [1, K]

    def forward(self, logits, true_ages):
        """
        logits: [B, K]
        true_ages: [B]
        """
        # Ordinal Regression label encoding
        # Example: Age 3, K=5. Label=[1, 1, 1, 0, 0]
        # P(y > k) should be 1 if true_age > k
        
        # true_ages: [B, 1]
        t_ages = true_ages.unsqueeze(1)
        
        # Binary Targets: [B, K]
        # rank_indices is [1, K]
        # target[i, k] = 1 if true_age[i] > k else 0
        targets = (t_ages > self.rank_indices).float()
        
        # æˆ‘ä»¬å¸Œæœ› Logits èƒ½å¤Ÿåæ˜ è¿™ç§ binary æ¦‚ç‡
        # ä½† Model è¾“å‡ºçš„æ˜¯ Softmax Logitsï¼Œå¹¶ä¸ç›´æ¥å¯¹åº” Binary Classifiers
        # è¿™é‡Œä½¿ç”¨ä¸€ç§ Proxyï¼š Cumulative Sum of Softmax? No.
        
        # æ›´å¥½çš„ Ranking Loss (Niu et al. CVPR 2016) éœ€è¦æ¨¡å‹è¾“å‡º 2K ä¸ªå€¼çš„ Binary Logits
        # è¿™é‡Œæˆ‘ä»¬çš„æ¨¡å‹æ˜¯æ ‡å‡†çš„ Softmax Multi-class
        # æ‰€ä»¥æˆ‘ä»¬æ”¹ç”¨: "Soft Softmax Ranking" 
        # æƒ©ç½š: å¦‚æœ P(k) é«˜ï¼Œé‚£ä¹ˆ P(k-1) around it ä¹Ÿåº”è¯¥åˆç†ï¼Œ
        # æ›´ç›´æ¥çš„æ˜¯ï¼šç›´æ¥æƒ©ç½š Expectation çš„ L1 (å·²ç»åœ¨ CombinedLoss é‡Œäº†)
        
        # ---- ä¿®æ­£æ–¹æ¡ˆ ----
        # é‰´äºåªä¿®æ”¹ Loss ä¸æ”¹æ¨¡å‹ç»“æ„ (Model è¾“å‡º output=num_classes)
        # æˆ‘ä»¬ä½¿ç”¨ "Expectation Ranking" æ— éœ€é¢å¤– Lossï¼ŒL1 å·²ç»åšäº†ã€‚
        # è¿™é‡Œå¦‚æœä¸€å®šè¦åŠ  Rank Lossï¼Œé€šå¸¸æ˜¯æŒ‡ Pairwise Ranking
        # éšæœºæŠ½å– Pairs (i, j)ï¼Œå¦‚æœ age_i > age_jï¼Œåˆ™ expectation_i > expectation_j + margin
        
        preds_age = torch.sum(F.softmax(logits, dim=1) * self.rank_indices, dim=1)
        
        # Pairwise Ranking
        n = preds_age.size(0)
        # éšæœºé‡‡æ ·ä¸€äº› pairs (ä¸ºäº†æ•ˆç‡ï¼Œä¸å…¨é‡)
        # ä¹Ÿå¯ä»¥ç®€å•çš„: Shuffle and Compare
        idx = torch.randperm(n).to(logits.device)
        preds_shuffled = preds_age[idx]
        ages_shuffled = true_ages[idx]
        
        # diff truth
        diff_truth = true_ages - ages_shuffled
        # diff pred
        diff_pred = preds_age - preds_shuffled
        
        # å¦‚æœ truthA > truthB (diff > 0), hope predA > predB (diff > 0)
        # Loss = ReLU( - sign(diff_truth) * diff_pred ) ?
        # ç®€å•ç‚¹: Sign Consistency
        # Loss = max(0, - sign(diff_truth) * diff_pred + margin)? 
        # No, for regression, L1 is optimal. 
        
        # å›å½’åˆ°åŸè®ºæ–‡æ€æƒ³ (DLDL + Ranking?)
        # è¿™é‡Œçš„ Rank Loss å¦‚æœæ˜¯ "Auxiliary"ï¼Œé€šå¸¸æ˜¯æŒ‡ Dr. DLDL æåˆ°çš„
        # "K-1 Binary Classifiers" (éœ€è¦ä¿®æ”¹æ¨¡å‹æœ€åä¸€å±‚)
        
        # æ—¢ç„¶æˆ‘ä»¬ä¸æƒ³æ”¹æ¨¡å‹ç»“æ„ï¼Œæˆ‘ä»¬ä¿ç•™è¿™ä¸ª Placeholder ä¸º L1 Loss çš„å¢å¼ºç‰ˆ
        # æˆ–è€…ä½¿ç”¨ "Distribution Cumulative Loss" -> CDF Loss
        # Minimize KL between CDFs (Cumulative Distribution Functions)
        # Earth Mover's Distance (EMD) è¿‘ä¼¼
        
        # CDF Calculation
        probs = F.softmax(logits, dim=1)
        cdf_pred = torch.cumsum(probs, dim=1)
        
        # True CDF (Target Distribution çš„ CDF)
        # æˆ‘ä»¬å¯ä»¥ç›´æ¥ç”¨ Heaviside Step Function on True Age?
        # æˆ–è€… DLDL Target çš„ CDF
        # ç®€å•èµ·è§ï¼Œç”¨ True Age çš„ Heaviside
        cdf_target = utils_cdf(true_ages, self.num_classes, logits.device)
        
        # EMD Loss (approx by L1 of CDFs)
        loss_emd = F.mse_loss(cdf_pred, cdf_target) # MSE on CDF is stricter
        return loss_emd

def utils_cdf(age, num_classes, device):
    # helper: create heaviside CDF
    # age [B], output [B, K]
    indices = torch.arange(num_classes, device=device).unsqueeze(0)
    # CDF: 1 if idx < age, else 0? No.
    # CDF(k) = P(X <= k)
    # if true_age = 3.5. 
    # k=0 (<=3.5? Yes), k=1(Yes)... k=3(Yes), k=4(No)
    # So 1 if k < true_age ? 
    # Typically CDF is 1 for k >= true_age.
    # Let takes floor.
    mask = (indices >= age.unsqueeze(1)).float() 
    # target CDF: 0 0 0 1 1 1 ... (step at age)
    # Actually: 0 0 0 ... until age, then 1.
    return (indices >= age.unsqueeze(1)).float()

class CombinedLoss(nn.Module):
    def __init__(self, config, weights=None):
        super(CombinedLoss, self).__init__()
        self.kl_loss = nn.KLDivLoss(reduction='batchmean')
        self.lambda_l1 = getattr(config, 'lambda_l1', 0.1)
        self.lambda_rank = getattr(config, 'lambda_rank', 0.5) # è·å– rank weight
        
        self.dldl = DLDLProcessor(config)
        self.weights = weights 
        
        # ä½¿ç”¨ CDF loss ä½œä¸º "Rank/Structure" Loss
        # Only init if use_dldl_v2 is True
        self.use_dldl_v2 = getattr(config, 'use_dldl_v2', True)
        if self.use_dldl_v2:
            self.rank_loss_fn = OrderRegressionLoss(config)
            # print("âœ… [Loss] Ranking Loss: ENABLED")
        else:
            self.rank_loss_fn = None
            # print("â„¹ï¸ [Loss] Ranking Loss: DISABLED (Standard L1+KL)")

    def forward(self, log_probs, target_dists, true_ages, logits):
        # 1. KL æ•£åº¦ (Main Loss)
        kl = self.kl_loss(log_probs, target_dists)
        
        # 2. Re-weighting (LDS)
        if self.weights is not None:
            element_kl = F.kl_div(log_probs, target_dists, reduction='none').sum(dim=1)
            age_indices = true_ages.long().clamp(0, len(self.weights)-1)
            batch_weights = self.weights[age_indices]
            w_kl = (element_kl * batch_weights).mean()
        else:
            w_kl = kl
            
        # 3. L1 Loss (Auxiliary)
        probs = torch.exp(log_probs)
        pred_age = self.dldl.expectation_regression(probs)
        l1 = F.l1_loss(pred_age, true_ages)
        
        # 4. Rank Loss (CDF Loss / EMD)
        # æ³¨æ„: OrderRegressionLoss å†…éƒ¨å®ç°äº† CDF MSE
        if self.use_dldl_v2 and self.rank_loss_fn is not None:
            rank_loss = self.rank_loss_fn(logits, true_ages)
            term_rank = self.lambda_rank * rank_loss
        else:
            rank_loss = torch.tensor(0.0).to(log_probs.device)
            term_rank = 0.0
        
        # æ€»æŸå¤±
        total_loss = w_kl + self.lambda_l1 * l1 + term_rank
        return total_loss, w_kl.item(), l1.item(), rank_loss.item()
