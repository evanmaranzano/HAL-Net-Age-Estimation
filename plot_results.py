import pandas as pd
import matplotlib.pyplot as plt
import os
import numpy as np
import seaborn as sns

# ================= é…ç½®åŒºåŸŸ =================
SAVE_DIR = 'plots'
plt.style.use('seaborn-v0_8-paper')

# ç»Ÿä¸€çš„è®ºæ–‡çº§å­—ä½“é…ç½®
plt.rcParams.update({
    'font.family': 'serif',
    'font.serif': ['Times New Roman'],
    'axes.unicode_minus': False,
    'figure.dpi': 300,
    'axes.labelsize': 14,
    'axes.titlesize': 16,
    'axes.titleweight': 'bold',
    'legend.fontsize': 12,
    'xtick.labelsize': 11,
    'ytick.labelsize': 11,
    'lines.linewidth': 2.5 
})

def load_real_data():
    if not (os.path.exists('training_log.csv') and os.path.exists('batch_log.csv')):
        print("âŒ é”™è¯¯: æœªæ‰¾åˆ° CSV æ–‡ä»¶")
        return None, None
    print("ğŸ“‚ è¯»å–æ•°æ®ä¸­...")
    df_epoch = pd.read_csv('training_log.csv')
    df_batch = pd.read_csv('batch_log.csv')
    return df_epoch, df_batch

def add_best_model_line(ax_plt, epoch, label_y_pos=None, color='#333333'):
    """è¾…åŠ©å‡½æ•°ï¼šæ·»åŠ æœ€ä½³æ¨¡å‹å‚ç›´çº¿"""
    ax_plt.axvline(x=epoch, color=color, linestyle='--', linewidth=1.5, alpha=0.7)
    
    ymin, ymax = ax_plt.get_ylim()
    text_pos = ymax - (ymax - ymin) * 0.05 if label_y_pos is None else label_y_pos
    
    ax_plt.text(epoch, text_pos, ' Best Checkpoint', rotation=90, 
                verticalalignment='top', fontsize=10, color=color, alpha=0.8)

def plot_thesis_suite():
    if not os.path.exists(SAVE_DIR):
        os.makedirs(SAVE_DIR)
        
    df_epoch, df_batch = load_real_data()
    if df_epoch is None: return

    # è®¡ç®—å…¨å±€æœ€ä½³è½®æ¬¡ (åŸºäº Val_MAE)
    best_idx = df_epoch['Val_MAE'].idxmin()
    best_epoch = df_epoch.loc[best_idx, 'Epoch']
    best_mae_val = df_epoch.loc[best_idx, 'Val_MAE']
    
    print(f"ğŸš€ å¼€å§‹ç”Ÿæˆ 8 å¼ ç‹¬ç«‹å›¾è¡¨ -> {SAVE_DIR}/")
    print(f"ğŸ’¡ æœ€ä½³æ¨¡å‹å‡ºç°åœ¨ç¬¬ {best_epoch} è½® (MAE={best_mae_val:.4f})")

    # ==========================================
    # å›¾ 1: Loss æ”¶æ•›æ›²çº¿ (åŸºç¡€ç‰ˆ)
    # ==========================================
    plt.figure(figsize=(8, 6))
    ax1 = plt.gca()
    plt.plot(df_epoch['Epoch'], df_epoch['Train_Loss'], label='Train Loss', color='#2878B5')
    plt.plot(df_epoch['Epoch'], df_epoch['Val_Loss'], label='Val Loss', color='#D76364', linestyle='--')
    add_best_model_line(ax1, best_epoch)
    plt.title('Loss Convergence')
    plt.xlabel('Epoch')
    plt.ylabel('Loss')
    plt.legend(frameon=True, fancybox=True)
    plt.grid(True, linestyle='--', alpha=0.5)
    plt.tight_layout()
    plt.savefig(f'{SAVE_DIR}/1_loss_curve.png')
    plt.close()

    # ==========================================
    # å›¾ 2: MAE æ€§èƒ½æ›²çº¿
    # ==========================================
    plt.figure(figsize=(8, 6))
    plt.plot(df_epoch['Epoch'], df_epoch['Train_MAE'], label='Train MAE', color='#9AC9DB')
    plt.plot(df_epoch['Epoch'], df_epoch['Val_MAE'], label='Val MAE', color='#C82423', linestyle='--')
    plt.axvline(x=best_epoch, color='gray', linestyle='--', linewidth=1.5, alpha=0.6)
    plt.scatter(best_epoch, best_mae_val, color='black', s=60, zorder=5)
    plt.annotate(f'Best MAE: {best_mae_val:.2f}\n(Epoch {best_epoch})', 
                 xy=(best_epoch, best_mae_val), 
                 xytext=(best_epoch + 5, best_mae_val + 0.5),
                 arrowprops=dict(facecolor='black', arrowstyle='->'),
                 fontsize=12, fontweight='bold',
                 bbox=dict(boxstyle='round,pad=0.2', fc='white', alpha=0.8))
    plt.title('Model Performance (MAE)')
    plt.xlabel('Epoch')
    plt.ylabel('Mean Absolute Error')
    plt.legend(frameon=True, fancybox=True)
    plt.grid(True, linestyle='--', alpha=0.5)
    plt.tight_layout()
    plt.savefig(f'{SAVE_DIR}/2_mae_curve.png')
    plt.close()

    # ==========================================
    # å›¾ 3: å­¦ä¹ ç‡è°ƒåº¦
    # ==========================================
    plt.figure(figsize=(8, 4))
    plt.plot(df_epoch['Epoch'], df_epoch['LR'], color='#6D6D6D', alpha=0.8)
    plt.fill_between(df_epoch['Epoch'], df_epoch['LR'], color='#6D6D6D', alpha=0.1)
    plt.axvline(x=best_epoch, color='gray', linestyle=':', linewidth=1, alpha=0.5)
    plt.title('Learning Rate Schedule')
    plt.xlabel('Epoch')
    plt.ylabel('Learning Rate (log scale)')
    plt.yscale('log')
    plt.grid(True, which="both", ls="--", alpha=0.3)
    plt.tight_layout()
    plt.savefig(f'{SAVE_DIR}/3_lr_schedule.png')
    plt.close()

    # ==========================================
    # å›¾ 4: æ³›åŒ–å·®è·åˆ†æ
    # ==========================================
    gap = df_epoch['Val_Loss'] - df_epoch['Train_Loss']
    plt.figure(figsize=(8, 5))
    ax4 = plt.gca()
    plt.plot(df_epoch['Epoch'], gap, color='#845EC2', label='Generalization Gap')
    plt.fill_between(df_epoch['Epoch'], gap, 0, color='#845EC2', alpha=0.15)
    z = np.polyfit(df_epoch['Epoch'], gap, 1)
    p = np.poly1d(z)
    plt.plot(df_epoch['Epoch'], p(df_epoch['Epoch']), "k--", alpha=0.5, linewidth=1, label='Gap Trend')
    add_best_model_line(ax4, best_epoch)
    plt.title('Generalization Gap Dynamics')
    plt.xlabel('Epoch')
    plt.ylabel('Loss Difference ($Val - Train$)')
    plt.legend(loc='upper left')
    plt.grid(True, linestyle='--', alpha=0.5)
    plt.tight_layout()
    plt.savefig(f'{SAVE_DIR}/4_generalization_gap.png')
    plt.close()

    # ==========================================
    # å›¾ 5: Batch ç¨³å®šæ€§ (è¶‹åŠ¿å›¾)
    # ==========================================
    plt.figure(figsize=(12, 4))
    global_steps = range(len(df_batch))
    plt.plot(global_steps, df_batch['Total_Loss'], color='#555555', alpha=0.3, linewidth=0.5, label='Raw Batch Loss')
    window = 100
    if len(df_batch) > window:
        trend = df_batch['Total_Loss'].rolling(window).mean()
        plt.plot(global_steps, trend, color='#C82423', linewidth=1.5, label=f'Trend (MA={window})')
    plt.title('Training Stability (Batch Level)')
    plt.xlabel('Global Step')
    plt.ylabel('Loss')
    limit = df_batch['Total_Loss'].iloc[int(len(df_batch)*0.01):].quantile(0.999) * 1.1
    plt.ylim(0, limit)
    plt.legend(loc='upper right', frameon=True)
    plt.margins(x=0)
    plt.tight_layout()
    plt.savefig(f'{SAVE_DIR}/5_batch_stability.png')
    plt.close()

    # ==========================================
    # [NEW] å›¾ 6: è®­ç»ƒæ—¶é—´æ•ˆç‡åˆ†æ (Time Efficiency)
    # ==========================================
    # è®¡ç®—æ¯ä¸ª Epoch çš„è€—æ—¶ (å¤„ç†æ–­ç‚¹ç»­è®­çš„æƒ…å†µ)
    time_deltas = []
    prev_time = 0
    print("\nâ±ï¸ æ­£åœ¨åˆ†æè®­ç»ƒè€—æ—¶ (æ£€æµ‹æ–­ç‚¹)...")
    for idx, t in enumerate(df_epoch['Time']):
        epoch_num = df_epoch.loc[idx, 'Epoch']
        if t < prev_time: # å‘ç”Ÿäº†é‡å¯
            delta = t
            print(f"  -> Epoch {epoch_num}: æ£€æµ‹åˆ°æ—¶é—´é‡ç½® (Time={t:.1f}s) -> åˆ¤å®šä¸ºé‡å¯åé¦–è½®")
        else:
            delta = t - prev_time
        
        time_deltas.append(delta)
        prev_time = t
    
    avg_time = np.mean(time_deltas)
    print(f"  -> å¹³å‡æ¯è½®è€—æ—¶: {avg_time:.2f} ç§’")

    plt.figure(figsize=(8, 5))
    plt.plot(df_epoch['Epoch'], time_deltas, marker='o', markersize=4, color='#2E8B57', alpha=0.8)
    plt.axhline(y=avg_time, color='#2E8B57', linestyle='--', alpha=0.5, label=f'Avg: {avg_time:.1f}s')
    plt.title('Training Time Cost per Epoch')
    plt.xlabel('Epoch')
    plt.ylabel('Duration (seconds)')
    plt.legend()
    plt.grid(True, linestyle='--', alpha=0.5)
    plt.tight_layout()
    plt.savefig(f'{SAVE_DIR}/6_time_efficiency.png')
    plt.close()

    # ==========================================
    # [NEW] å›¾ 7: Batch Loss åˆ†å¸ƒ (Boxplot)
    # ==========================================
    # å±•ç¤ºæ¯ä¸ª Epoch çš„ Loss åˆ†å¸ƒï¼Œè§‚å¯Ÿæ”¶æ•›çš„æ–¹å·®å˜åŒ–
    plt.figure(figsize=(10, 6))
    # ä¸ºäº†é˜²æ­¢ Epoch å¤ªå¤šå¯¼è‡´ç®±çº¿å›¾å¤ªæŒ¤ï¼Œæˆ‘ä»¬æ¯éš”å‡ ä¸ª Epoch é‡‡æ ·ä¸€ä¸ªï¼Œæˆ–è€…åªç”»å‰Nå’ŒåN
    # è¿™é‡Œé€‰æ‹©ï¼šå¦‚æœ Epoch < 20 å…¨ç”»ï¼Œå¦åˆ™æ¯éš” (Total/20) ç”»ä¸€ä¸ª
    unique_epochs = df_batch['Epoch'].unique()
    if len(unique_epochs) > 20:
        step = len(unique_epochs) // 20
        selected_epochs = unique_epochs[::step]
    else:
        selected_epochs = unique_epochs
    
    filtered_batch = df_batch[df_batch['Epoch'].isin(selected_epochs)]
    
    # ä¿®å¤ï¼šæ·»åŠ  hue å‚æ•°å’Œ legend=False
    sns.boxplot(x='Epoch', y='Total_Loss', data=filtered_batch, hue='Epoch', palette="Blues", fliersize=1, linewidth=1, legend=False)
    plt.title('Batch Loss Distribution per Epoch (Variance Analysis)')
    plt.xlabel('Epoch')
    plt.ylabel('Batch Loss')
    plt.grid(axis='y', linestyle='--', alpha=0.5)
    plt.tight_layout()
    plt.savefig(f'{SAVE_DIR}/7_batch_loss_dist.png')
    plt.close()

    # ==========================================
    # [NEW] å›¾ 8: Loss ä¸ LR è”åˆåˆ†æ (Dual Axis)
    # ==========================================
    fig, ax1_dual = plt.subplots(figsize=(9, 6))
    
    color_loss = '#D76364'
    ax1_dual.set_xlabel('Epoch')
    ax1_dual.set_ylabel('Val Loss', color=color_loss)
    ax1_dual.plot(df_epoch['Epoch'], df_epoch['Val_Loss'], color=color_loss, label='Val Loss', linewidth=2)
    ax1_dual.tick_params(axis='y', labelcolor=color_loss)
    
    ax2_dual = ax1_dual.twinx()  # å®ä¾‹åŒ–ç¬¬äºŒä¸ªè½´
    color_lr = '#6D6D6D'
    ax2_dual.set_ylabel('Learning Rate', color=color_lr)
    ax2_dual.plot(df_epoch['Epoch'], df_epoch['LR'], color=color_lr, linestyle='--', alpha=0.6, label='LR')
    ax2_dual.tick_params(axis='y', labelcolor=color_lr)
    ax2_dual.set_yscale('log')

    plt.title('Validation Loss vs Learning Rate')
    fig.tight_layout()
    plt.savefig(f'{SAVE_DIR}/8_loss_lr_combined.png')
    plt.close()

    print("\nğŸ‰ å…¨éƒ¨ 8 å¼ å›¾è¡¨ç”Ÿæˆå®Œæ¯•ï¼å·²æ¦¨å¹² CSV çš„å…¨éƒ¨æ½œåŠ›ï¼")

if __name__ == '__main__':
    plot_thesis_suite()